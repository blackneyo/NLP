{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1b8433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from nltk) (2022.4.24)\n",
      "Requirement already satisfied: click in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from nltk) (8.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\4lab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_text = \"A dog run back corner near spare bedrooms\"\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bb6183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'dog', 'run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a44b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'dog', 'run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "print(en_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5793d3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과의', '놀라운', '효능이라는', '글을', '봤어.', '그래서', '오늘', '사과를', '먹으려고', '했는데', '사과가', '썩어서', '슈퍼에', '가서', '사과랑', '오렌지', '사왔어']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n위의 예제에서는 사과란 단어가 총 4번 등장\\n'의,를,가,랑' 이 붙어있어서 이를 제거해주지 않으면 기게는 전부 다른 단어로 인식\\n따라서, 이를 위해 한국어는 보편적으로 형태소 분석기로 토큰화 시행 // mecab을 사용할 예정\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\"\n",
    "print(kor_text.split())\n",
    "'''\n",
    "위의 예제에서는 사과란 단어가 총 4번 등장\n",
    "'의,를,가,랑' 이 붙어있어서 이를 제거해주지 않으면 기게는 전부 다른 단어로 인식\n",
    "따라서, 이를 위해 한국어는 보편적으로 형태소 분석기로 토큰화 시행 // mecab을 사용할 예정\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b390401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicpath라는 Parameter 지정\n",
    "# Mecab의 형태소 단위 토크나이징 수행 후 토큰화 진행\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "623af925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아버지', '가', '방', '에', '들어가', '신다']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab.morphs(\"아버지가방에들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b60440d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가', 'JKS'),\n",
       " ('방', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('들어가', 'VV'),\n",
       " ('신다', 'EP+EC')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 태깅\n",
    "mecab.pos(\"아버지가방에들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1d8c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n",
    "\n",
    "def get_tokenizer(tokenizer_name):\n",
    "    if tokenizer_name == \"komoran\":\n",
    "        tokenizer = Komoran()\n",
    "    elif tokenizer_name == \"okt\":\n",
    "        tokenizer = Okt()\n",
    "    elif tokenizer_name == \"mecab\":\n",
    "        tokenizer = Mecab(dicpath=r\"c:\\mecab\\mecab-ko-dic\")\n",
    "    elif tokenizer_name == \"hannanum\":\n",
    "        tokenizer = Hannanum()\n",
    "    elif tokenizer_name == \"kkma\":\n",
    "        tokenizer = Kkma()\n",
    "    else:\n",
    "        tokenizer = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30bec849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  토크나이저  토큰화 소요시간\n",
      "0  kkma   1.15277\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "time_list = []\n",
    "tokenizer_list = [\"komoran\", \"okt\", \"mecab\", \"hannanum\", \"kkma\"]\n",
    "\n",
    "for i in tokenizer_list:\n",
    "    start = time.time() \n",
    "    tokenizer = get_tokenizer(i)\n",
    "    tokenizer.morphs(\"\"\"한국어 자연어 처리 분야에 정말 괜찮은 책이 나왔다\n",
    "    출간되자 마자 읽어 보고 저자의 내공에 참 놀랐다\n",
    "    이렇게 좋은 품질의 도서가 더 많이 나와야 한다고 생각하는 입장에서 저자에게 고마울 따름이다\n",
    "    하지만 네이버 책정보에도 YES24에도 이제까지 연결된 리뷰가 하나도 없다는 점이 좀 의아했다\n",
    "    네이버 블로그 리뷰는 좀 있음에도 그 이유는 아마도 책 제목 때문이 아닐까 싶었다\n",
    "    아무래도 입문자 분들이 많이 봐야 할텐데 인공지능이나 딥러닝 같은 키워드가 아닌 한국어 임베딩이라고 제목이 달려있으니\n",
    "    뭔가 다른 기술이거나 아니면 매우 작은 범위의 기술로 착각할 수 있기 때문이다\n",
    "    임베딩은 자연어 처리 뿐만 아니라 컴퓨터 비전 음성에서도 매우 중요한 키워드가 됐다\n",
    "    나 같은 경우 딥러닝 공부 초창기에 GAN에서 임베딩이라는 개념을 처음 접하게 되었는데 이걸 이해하려고 고생했던 기억이 있다\n",
    "    딥러닝을 공부하면 할 수록 임베딩이라는 용어는 여러 개념을 관통하는 너무나 중요한 용어다.\n",
    "    이 책은 최신 자연어 처리 기술을 포함하고 있으며 이를 한국어에 맞게 적용할 수 있도록 일목요연하게 설명한 책이다\n",
    "    특히 저자의 언어로 이러한 기술들을 설명한 점이 가장 중요하다\n",
    "    최근 자연어 처리 기술이 BERT를 통해 퀀텀 점프를 하였고 XLNet과 같은 후속 연구가 빠르게 진행되면서 급속하게 발전하고 있다\n",
    "    저자는 BERT까지만 다루었는데 XLNet 같은 경우 직접 실험해보니 BERT보다 성능이 부족하다 판단하여 제외했다는 언급이 있다\n",
    "    이러한 부분이 책의 신뢰성을 더 높인다고 생각한다\n",
    "    직접 실험해보고 고민해보고 이해한 흔적이 저자만의 언어로 설명되면 독자는 더 많은 통찰을 얻을 수 있다\n",
    "    데이터를 다루는 부분부터 소스코드도 괜찮고 그림 설명도 훌륭하다\n",
    "    다만 코드를 마이크로하게 설명하는 부분은 부족하여 입문자 분들에겐 힘들수도 있지만 그래도 중요한 부분은 모두 언급했기 때문에 나쁘지 않다고 생각한다\n",
    "    한국어 용어도 내 입장에서는 매우 마음에 들었다\n",
    "    영어 발음 그대로 한국어로 쓰는 것을 싫어하시는 분들도 계시겠지만 결국 최신 기술은 영어 논문을 읽어야 하기 때문에 어설프게 한국어로 번역된 용어 보다 훨씬 낫다고 생각한다\n",
    "    처음 나오는 전문용어 옆에는 영어로 표시하여 헷갈리지 않도록 충분히 배려했다\n",
    "    제발 다른 번역서도 이 책을 참고하여 어설프게 한국어로 번역하지 않았으면 하는 바람이다\n",
    "    자연어 처리의 딥러닝 기술을 매우 잘 설명한 좋은 책이다\n",
    "    아직 리뷰가 하나도 없지만 더 많은 리뷰도 달리고 더 잘 팔려서 최신 기술을 담은 2판이 나오길 희망한다\"\"\")\n",
    "\n",
    "time_required = time.time() - start\n",
    "tokenizer_and_time = i, time_required\n",
    "\n",
    "time_list.append(tokenizer_and_time)\n",
    "소요시간측정 = pd.DataFrame(time_list, columns = ['토크나이저', '토큰화 소요시간'])\n",
    "\n",
    "print(소요시간측정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e72c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과', '의', '놀라운', '효능', '이', '라는', '글', '을', '봤', '어', '.', '그래서', '오늘', '사과', '를', '먹', '으려고', '했', '는데', '사과', '가', '썩', '어서', '슈퍼', '에', '가', '서', '사과', '랑', '오렌지', '사', '왔', '어']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab(dicpath = r\"C:\\mecab\\mecab-ko-dic\")\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b5cfa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', ' ', 'd', 'o', 'g', ' ', 'r', 'u', 'n', ' ', 'b', 'a', 'c', 'k', ' ', 'c', 'o', 'r', 'n', 'e', 'r', ' ', 'n', 'e', 'a', 'r', ' ', 's', 'p', 'a', 'r', 'e', ' ', 'b', 'e', 'd', 'r', 'o', 'o', 'm', 's']\n"
     ]
    }
   ],
   "source": [
    "print(list(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a2cb155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a33c998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2190435</td>\n",
       "      <td>사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9279041</td>\n",
       "      <td>완전 감동입니다 다시봐도 감동</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7865729</td>\n",
       "      <td>개들의 전쟁2 나오나요? 나오면 1빠로 보고 싶음</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7477618</td>\n",
       "      <td>굿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9250537</td>\n",
       "      <td>바보가 아니라 병 쉰 인듯</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
       "5   2190435                      사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화      1\n",
       "6   9279041                                   완전 감동입니다 다시봐도 감동      1\n",
       "7   7865729                        개들의 전쟁2 나오나요? 나오면 1빠로 보고 싶음      1\n",
       "8   7477618                                                  굿      1\n",
       "9   9250537                                     바보가 아니라 병 쉰 인듯      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 단어 집합 생성\n",
    "### 단어 집합이란 중복을 제거한 텍스트의 총 단어의 집합(set)을 의미\n",
    "### 깃허브에 '네이버 영화 리뷰 분류하기' 데이터 다운로드\n",
    "### 총 20만개 영화 리뷰를 긍정1, 부정0 으로 레이블링\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "data = pd.read_table('ratings.txt') # 데이터프레임에 저장\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0123a460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 200000\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 : {}'.format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1cf0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의로 100개만 저장\n",
    "sample_data = data[:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e499b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\4lab\\AppData\\Local\\Temp\\ipykernel_26444\\749348335.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "C:\\Users\\4lab\\AppData\\Local\\Temp\\ipykernel_26444\\749348335.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2190435</td>\n",
       "      <td>사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9279041</td>\n",
       "      <td>완전 감동입니다 다시봐도 감동</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7865729</td>\n",
       "      <td>개들의 전쟁 나오나요 나오면 빠로 보고 싶음</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7477618</td>\n",
       "      <td>굿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9250537</td>\n",
       "      <td>바보가 아니라 병 쉰 인듯</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1\n",
       "5   2190435                      사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화      1\n",
       "6   9279041                                   완전 감동입니다 다시봐도 감동      1\n",
       "7   7865729                           개들의 전쟁 나오나요 나오면 빠로 보고 싶음      1\n",
       "8   7477618                                                  굿      1\n",
       "9   9250537                                     바보가 아니라 병 쉰 인듯      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규 표현식을 통해서 데이터를 정제\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "sample_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2daa3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 // 토큰화 과정에서 불용어를 제거하기 위해 불용어를 먼저 정의\n",
    "# 불용어 처리\n",
    "stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b08d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab(dicpath = r\"C:\\mecab\\mecab-ko-dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52bc1f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ'], ['디자인', '을', '배우', '학생', '외국', '디자이너', '그', '일군', '전통', '을', '통해', '발전', '해', '문화', '산업', '부러웠', '는데', '사실', '우리', '나라', '에서', '그', '어려운', '시절', '끝', '까지', '열정', '을', '지킨', '노라노', '같', '전통', '있', '어', '저', '같', '사람', '꿈', '을', '꾸', '고', '이뤄나갈', '수', '있', '다는', '것', '감사', '합니다'], ['폴리스', '스토리', '시리즈', '부터', '뉴', '까지', '버릴', '께', '하나', '없', '음', '최고'], ['연기', '진짜', '개', '쩔', '구나', '지루', '할거', '라고', '생각', '했', '는데', '몰입', '해서', '봤', '다', '그래', '이런', '게', '진짜', '영화', '지'], ['안개', '자욱', '밤하늘', '떠', '있', '초승달', '같', '영화'], ['사랑', '을', '해', '본', '사람', '라면', '처음', '부터', '끝', '까지', '웃', '을', '수', '있', '영화'], ['완전', '감동', '입니다', '다시', '봐도', '감동'], ['개', '전쟁', '나오', '나요', '나오', '면', '빠', '로', '보', '고', '싶', '음'], ['굿'], ['바보', '아니', '라', '병', '쉰', '인', '듯']]\n"
     ]
    }
   ],
   "source": [
    "tokenized = []\n",
    "for sentence in sample_data['document']:\n",
    "    temp = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp = [word for word in temp if not word in stopwords] #불용어 제거\n",
    "    tokenized.append(temp)\n",
    "print(tokenized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d451d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 664\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합 생성 NLTK에서 빈도수 계산 도구 FreqDist 지원\n",
    "vocab = FreqDist(np.hstack(tokenized))\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b973b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['재밌']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9b891d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 500\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 500\n",
    "# 상위 vocab_size개의 단어만 보존\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97e7be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어에 고유한 정수 부여\n",
    "# enumerate() 는 순서가 있는 자료형 (list, set, tuple, dictionary, string)을 입력받아 인덱스를 순차적으로 함께 리턴\n",
    "# 인덱스 0과 1을 다른용도로 남기고 나머지 단어들을 2~501까지 순차적으로 인덱스 부여\n",
    "\n",
    "word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
    "word_to_index[ 'pad' ] = 1\n",
    "word_to_index[ 'unk' ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7aa82a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78, 27, 9, 4, 50, 41, 79, 16, 28, 29], [188, 5, 80, 189, 190, 191, 42, 192, 114, 5, 193, 194, 21, 115, 195, 196, 13, 51, 81, 116, 30, 42, 197, 117, 118, 31, 198, 5, 199, 200, 17, 114, 7, 82, 52, 17, 43, 201, 5, 202, 4, 203, 14, 7, 83, 32, 204, 84], [205, 119, 206, 53, 207, 31, 208, 209, 54, 10, 25, 11], [44, 33, 120, 210, 211, 212, 213, 68, 45, 34, 13, 214, 121, 15, 2, 215, 69, 8, 33, 3, 35], [216, 217, 218, 219, 7, 220, 17, 3], [122, 5, 21, 36, 43, 123, 124, 53, 118, 31, 85, 5, 14, 7, 3], [125, 37, 221, 41, 79, 37], [120, 222, 55, 223, 55, 86, 224, 46, 9, 4, 47, 25], [56], [225, 87, 88, 226, 227, 57, 89]]\n"
     ]
    }
   ],
   "source": [
    "# 기존 훈련 데이터에서 각 단어를 고유한 정수로 부여하는 작업 진행\n",
    "encoded = []\n",
    "for line in tokenized: # 입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line : # 각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "        except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "            \n",
    "    encoded.append(temp)\n",
    "print(encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba0755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
